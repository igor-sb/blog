{
 "cells": [
  {
   "cell_type": "raw",
   "id": "c6bd3dbe",
   "metadata": {},
   "source": [
    "---\n",
    "title: Multiple Hypothesis Testing\n",
    "format:\n",
    "  pdf:\n",
    "    code-fold: true\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfa6d961",
   "metadata": {},
   "source": [
    "Multiple hypothesis testing occurs when we repeatedly test models on a number of features, as the probability of obtaining one or more false discoveries increases with the number of tests. For example, in the field of genomics, scientists often want to test whether any of the thousands of genes have a significantly different activity in an outcome of interest.\n",
    "\n",
    "In this blog post, we will cover few of the popular methods used to account for multiple hypothesis testing by adjusting model p-values:\n",
    "\n",
    "1. False Positive Rate (FPR)\n",
    "2. Family-Wise Error Rate (FWER)\n",
    "3. False Discovery Rate (FDR)\n",
    "\n",
    " and explain when it makes sense to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d429d3",
   "metadata": {},
   "source": [
    "This document can be summarized in the following image:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd36110",
   "metadata": {},
   "source": [
    "<img src=\"images/pvalue_adjustments.png\" width=90% height=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f32cf",
   "metadata": {},
   "source": [
    "# Create test data\n",
    "\n",
    "We will create a simulated example to better understand how various manipulation of p-values can lead to different conclusions. To run this code, we need Python with `pandas`, `numpy`, `scipy` and `statsmodels` libraries installed.\n",
    "\n",
    "For the purpose of this example, we start by creating a dataframe of 1000 features. 990 of which (99%) will have their values generated from a Normal distribution with mean = 0, called a Null model. (In a function `norm.rvs()` used below, mean is set using a `loc` argument.) The remaining 1% of the features will be generated from a Normal distribution mean = 3, called a Non-Null model. We will use these as representing interesting features that we would like to discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5a532c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47784932",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_null = 9900\n",
    "n_nonnull = 100\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'hypothesis': np.concatenate((\n",
    "        ['null'] * n_null,\n",
    "        ['non-null'] * n_nonnull,\n",
    "    )),\n",
    "    'feature': range(n_null + n_nonnull),\n",
    "    'x': np.concatenate((\n",
    "        norm.rvs(loc=0, scale=1, size=n_null),\n",
    "        norm.rvs(loc=3, scale=1, size=n_nonnull),\n",
    "    ))\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af26527",
   "metadata": {},
   "source": [
    "For each of the 1000 features, p-value is a probability of observing the value at least as large, if we assume it was generated from a Null distribution.\n",
    "\n",
    "P-values can be calculating from cumulative distribution. Cumulative distribution from `scipy.stats`, is named `norm.cdf()` and represents the probability of obtaining a value equal to or **less than** the one observed. Then to calculate the p-value we calculate `1 - norm.cdf()` to find the probability **greater than** the one observed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "24f62269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>null</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496714</td>\n",
       "      <td>0.309695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>null</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.138264</td>\n",
       "      <td>0.554984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>null</td>\n",
       "      <td>2</td>\n",
       "      <td>0.647689</td>\n",
       "      <td>0.258593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>null</td>\n",
       "      <td>3</td>\n",
       "      <td>1.523030</td>\n",
       "      <td>0.063876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>null</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.234153</td>\n",
       "      <td>0.592567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9995</td>\n",
       "      <td>4.301102</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9996</td>\n",
       "      <td>1.001655</td>\n",
       "      <td>0.158255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9997</td>\n",
       "      <td>2.294683</td>\n",
       "      <td>0.010876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9998</td>\n",
       "      <td>3.495766</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9999</td>\n",
       "      <td>3.644388</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hypothesis  feature         x   p_value\n",
       "0          null        0  0.496714  0.309695\n",
       "1          null        1 -0.138264  0.554984\n",
       "2          null        2  0.647689  0.258593\n",
       "3          null        3  1.523030  0.063876\n",
       "4          null        4 -0.234153  0.592567\n",
       "...         ...      ...       ...       ...\n",
       "9995   non-null     9995  4.301102  0.000008\n",
       "9996   non-null     9996  1.001655  0.158255\n",
       "9997   non-null     9997  2.294683  0.010876\n",
       "9998   non-null     9998  3.495766  0.000236\n",
       "9999   non-null     9999  3.644388  0.000134\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['p_value'] = 1 - norm.cdf(df['x'], loc = 0, scale = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25ba91",
   "metadata": {},
   "source": [
    "# False Positive Rate\n",
    "\n",
    "The first concept is called a False Positive Rate, and is defined as a fraction of null hypothesis that we flag as \"significant\" (also called Type I errors). The p-values we calculated earlier can be interpreted as a false positive rate by their very definition: they are probabilities of obtaining a value at least as large as a specified value, when we sample a null distribution.\n",
    "\n",
    "For illustrative purposes, we will apply a common (magical) p-value threshold of 0.05, but any threshold can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4f5573f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hypothesis  is_raw_p_value_significant\n",
       "non-null    False                            8\n",
       "            True                            92\n",
       "null        False                         9407\n",
       "            True                           493\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_raw_p_value_significant'] = df['p_value'] <= 0.05\n",
    "df.groupby(['hypothesis', 'is_raw_p_value_significant']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbaf5e1",
   "metadata": {},
   "source": [
    "notice that out of our 9900 null hypotheses, 493 are flagged as \"significant\". Therefore, a False Positive Rate is: FPR =  493 / (493 + 9940) = 0.053. \n",
    "\n",
    "The main problem with FPR is that in a real scenario we do not a priori know which hypotheses are null and which are not. Then, the raw p-value on its own (False Positive Rate) can be of little use. In our case when the fraction of non-null features is very small, most of the features flagged as significant will be null, because there are many more of them. Specifically, out of 92 + 493 = 585 features flagged true (\"positive\"), only 92 are from our non-null distribution. That means that a majority or about 84% of reported significant features (493 / 585) are false positives!\n",
    "\n",
    "So what can we do about this? There are two common methods of addressing this issue: instead of False Positive Rate, we can calculate Family-Wise Error Rate (FWER) or a False Discovery Rate (FPR). Each of these methods takes the set of raw, unadjusted, p-values as an input, and produces a new set of \"adjusted p-values\" as an output. These \"adjusted p-values\" represent estimates of *upper bounds* on FWER and FDR. They can be obtained from `multipletests()` function, which is part of the `statsmodels` Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "feb5f292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_pvalues(p_values, method):\n",
    "   return multipletests(p_values, method = method)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb341fd",
   "metadata": {},
   "source": [
    "# Family-Wise Error Rate\n",
    "\n",
    "Family-Wise Error Rate is a probability of falsly rejecting one or more null hypothesis, i.e. flagging true Null as Non-null. In other words, this is a probability of obtaining one or more false positives. \n",
    "\n",
    "When there is only one hypothesis being tested, this is equal to the raw p-value (false positive rate). However, the more hypotheses are tested, the more likely we are going to get one or more false positives. There are two popular ways to estimate FWER: Bonferroni and Holm procedures. Although neither Bonferroni or Holm procedures make any assumptions about the dependence of tests run on individual features, they will be overly conservative. For example, in the extreme case when all of the features are identical (same model repeated 10,000 times), no correction is needed. While in the other extreme, where no features are correlated, some type of correction is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f6e3e",
   "metadata": {},
   "source": [
    "## Bonferroni procedure\n",
    "\n",
    "One of the most popular method for correcting for multiple hypothesis testing is a Bonferroni procedure. The reason this method is popular is because it is very easy to calculate, even by hand. This procedure multiplies each p-value by the total number of tests performed, or sets it to 1 if this multiplication would push it past 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93b6c019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "      <th>is_raw_p_value_significant</th>\n",
       "      <th>p_value_bonf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9907</td>\n",
       "      <td>5.322609</td>\n",
       "      <td>5.114466e-08</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9942</td>\n",
       "      <td>5.022174</td>\n",
       "      <td>2.554492e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9943</td>\n",
       "      <td>4.831177</td>\n",
       "      <td>6.786409e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9941</td>\n",
       "      <td>4.801528</td>\n",
       "      <td>7.872958e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9976</td>\n",
       "      <td>4.674271</td>\n",
       "      <td>1.475000e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.014750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>null</td>\n",
       "      <td>3336</td>\n",
       "      <td>-1.365824</td>\n",
       "      <td>9.140029e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>null</td>\n",
       "      <td>3337</td>\n",
       "      <td>-0.148969</td>\n",
       "      <td>5.592111e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3338</th>\n",
       "      <td>null</td>\n",
       "      <td>3338</td>\n",
       "      <td>0.502784</td>\n",
       "      <td>3.075579e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>null</td>\n",
       "      <td>3331</td>\n",
       "      <td>-1.545730</td>\n",
       "      <td>9.389151e-01</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9999</td>\n",
       "      <td>3.644388</td>\n",
       "      <td>1.340142e-04</td>\n",
       "      <td>True</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hypothesis  feature         x       p_value  is_raw_p_value_significant   \n",
       "9907   non-null     9907  5.322609  5.114466e-08                        True  \\\n",
       "9942   non-null     9942  5.022174  2.554492e-07                        True   \n",
       "9943   non-null     9943  4.831177  6.786409e-07                        True   \n",
       "9941   non-null     9941  4.801528  7.872958e-07                        True   \n",
       "9976   non-null     9976  4.674271  1.475000e-06                        True   \n",
       "...         ...      ...       ...           ...                         ...   \n",
       "3336       null     3336 -1.365824  9.140029e-01                       False   \n",
       "3337       null     3337 -0.148969  5.592111e-01                       False   \n",
       "3338       null     3338  0.502784  3.075579e-01                       False   \n",
       "3331       null     3331 -1.545730  9.389151e-01                       False   \n",
       "9999   non-null     9999  3.644388  1.340142e-04                        True   \n",
       "\n",
       "      p_value_bonf  \n",
       "9907      0.000511  \n",
       "9942      0.002554  \n",
       "9943      0.006786  \n",
       "9941      0.007873  \n",
       "9976      0.014750  \n",
       "...            ...  \n",
       "3336      1.000000  \n",
       "3337      1.000000  \n",
       "3338      1.000000  \n",
       "3331      1.000000  \n",
       "9999      1.000000  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['p_value_bonf'] = adjust_pvalues(df['p_value'], 'bonferroni')\n",
    "df.sort_values('p_value_bonf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756f00e",
   "metadata": {},
   "source": [
    "## Holm procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7fe29",
   "metadata": {},
   "source": [
    "Holm's procedure provides a correction that is more powerful than Bonferroni's procedure. The only difference is that the p-values are not all multiplied by the total number of tests (here, 10,000). Instead, each sorted p-value is multiplied progressively by a decreasing sequence 10,000, 9,999, 9,998, 9,997, ..., 3, 2, 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ae0f71b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "      <th>is_raw_p_value_significant</th>\n",
       "      <th>p_value_bonf</th>\n",
       "      <th>p_value_holm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9907</td>\n",
       "      <td>5.322609</td>\n",
       "      <td>5.114466e-08</td>\n",
       "      <td>True</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9942</td>\n",
       "      <td>5.022174</td>\n",
       "      <td>2.554492e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9943</td>\n",
       "      <td>4.831177</td>\n",
       "      <td>6.786409e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.006786</td>\n",
       "      <td>0.006785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9941</td>\n",
       "      <td>4.801528</td>\n",
       "      <td>7.872958e-07</td>\n",
       "      <td>True</td>\n",
       "      <td>0.007873</td>\n",
       "      <td>0.007871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9976</td>\n",
       "      <td>4.674271</td>\n",
       "      <td>1.475000e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.014750</td>\n",
       "      <td>0.014744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9964</td>\n",
       "      <td>4.589147</td>\n",
       "      <td>2.225301e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.022253</td>\n",
       "      <td>0.022242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9974</td>\n",
       "      <td>4.515318</td>\n",
       "      <td>3.161090e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.031611</td>\n",
       "      <td>0.031592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9990</td>\n",
       "      <td>4.433625</td>\n",
       "      <td>4.633087e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>0.046298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9909</td>\n",
       "      <td>4.414029</td>\n",
       "      <td>5.073215e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.050732</td>\n",
       "      <td>0.050692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9916</td>\n",
       "      <td>4.316007</td>\n",
       "      <td>7.943832e-06</td>\n",
       "      <td>True</td>\n",
       "      <td>0.079438</td>\n",
       "      <td>0.079367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hypothesis  feature         x       p_value  is_raw_p_value_significant   \n",
       "9907   non-null     9907  5.322609  5.114466e-08                        True  \\\n",
       "9942   non-null     9942  5.022174  2.554492e-07                        True   \n",
       "9943   non-null     9943  4.831177  6.786409e-07                        True   \n",
       "9941   non-null     9941  4.801528  7.872958e-07                        True   \n",
       "9976   non-null     9976  4.674271  1.475000e-06                        True   \n",
       "9964   non-null     9964  4.589147  2.225301e-06                        True   \n",
       "9974   non-null     9974  4.515318  3.161090e-06                        True   \n",
       "9990   non-null     9990  4.433625  4.633087e-06                        True   \n",
       "9909   non-null     9909  4.414029  5.073215e-06                        True   \n",
       "9916   non-null     9916  4.316007  7.943832e-06                        True   \n",
       "\n",
       "      p_value_bonf  p_value_holm  \n",
       "9907      0.000511      0.000511  \n",
       "9942      0.002554      0.002554  \n",
       "9943      0.006786      0.006785  \n",
       "9941      0.007873      0.007871  \n",
       "9976      0.014750      0.014744  \n",
       "9964      0.022253      0.022242  \n",
       "9974      0.031611      0.031592  \n",
       "9990      0.046331      0.046298  \n",
       "9909      0.050732      0.050692  \n",
       "9916      0.079438      0.079367  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['p_value_holm'] = adjust_pvalues(df['p_value'], 'holm')\n",
    "df.sort_values('p_value_holm').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c055df4",
   "metadata": {},
   "source": [
    "We can verify this ourselves: the last 10th p-value on this output is multiplied by 9991: 7.943832e-06 * 9991 = 0.079367. Holm's correction is also the default method for adjusting p-values in `p.adjust()` function in R language.\n",
    "\n",
    "If we again apply our p-value threshold of 0.05, let's take a look how these adjusted p-values affect our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c98a23e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hypothesis  is_p_value_holm_significant\n",
       "non-null    False                            92\n",
       "            True                              8\n",
       "null        False                          9900\n",
       "dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_p_value_holm_significant'] = df['p_value_holm'] <= 0.05\n",
    "df.groupby(['hypothesis', 'is_p_value_holm_significant']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e8fe6",
   "metadata": {},
   "source": [
    "These results are much different than when we applied the same threshold to the raw p-values! Here only 8 features are flagged as \"significant\", and all 8 are correct - they were generated from our Non-null distribution. This is because the probability of getting even one feature flagged incorrectly is only 0.05 (5%).\n",
    "\n",
    "However, this approach has another downside: it failed to flag other 92 Non-null features as significant. While it was very stringent to make sure no null features slipped in, it  was able to find only 8% (8 out of 100) non-null features. This seems to be a different extreme than the False Positive Rate approach.\n",
    "\n",
    "Is there a more middle ground? The answer is \"yes\", and that middle ground is False Discovery Rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a3fa04",
   "metadata": {},
   "source": [
    "# False Discovery Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07478fe",
   "metadata": {},
   "source": [
    "What if we are OK with letting some false positives in, but capturing much more than few percent of true positives?\n",
    "\n",
    "This can be done by controlling the False Discovery Rate (rather than FWER or FPR) at a specified threshold level, say 0.05. False Discovery Rate is defined a fraction of false positives among all features flagged as positive: FDR = FP / (FP + TP), where FP is the number of False Positives and TP is the number of True Positives.\n",
    "\n",
    "There are several methods to control FDR and here we will describe how to use two popular ones: Benjamini-Hochberg and Benjamini-Yekutieli procedures. Both of these procedures are similiar although more involved than FWER procedures. They still rely on sorting the p-values, multiplying them with a specific number, and then using a cutoff criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073fb850",
   "metadata": {},
   "source": [
    "## Benjamini-Hochberg procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48d15c",
   "metadata": {},
   "source": [
    "Benjamini-Hochberg (BH) procedure relies assumes that each of the tests are *independent*. Dependent tests occur, for example, if the features being testede are correlated with each other. Let's calculate the BH-adjusted p-values and compare it to our earlier result from FWER using Holm's correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb00a1cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "      <th>p_value_holm</th>\n",
       "      <th>p_value_bh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9907</td>\n",
       "      <td>5.322609</td>\n",
       "      <td>5.114466e-08</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9942</td>\n",
       "      <td>5.022174</td>\n",
       "      <td>2.554492e-07</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.001277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9941</td>\n",
       "      <td>4.801528</td>\n",
       "      <td>7.872958e-07</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9943</td>\n",
       "      <td>4.831177</td>\n",
       "      <td>6.786409e-07</td>\n",
       "      <td>0.006785</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9976</td>\n",
       "      <td>4.674271</td>\n",
       "      <td>1.475000e-06</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.002950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9964</td>\n",
       "      <td>4.589147</td>\n",
       "      <td>2.225301e-06</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.003709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9974</td>\n",
       "      <td>4.515318</td>\n",
       "      <td>3.161090e-06</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.004516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9990</td>\n",
       "      <td>4.433625</td>\n",
       "      <td>4.633087e-06</td>\n",
       "      <td>0.046298</td>\n",
       "      <td>0.005637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9909</td>\n",
       "      <td>4.414029</td>\n",
       "      <td>5.073215e-06</td>\n",
       "      <td>0.050692</td>\n",
       "      <td>0.005637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9995</td>\n",
       "      <td>4.301102</td>\n",
       "      <td>8.497538e-06</td>\n",
       "      <td>0.084890</td>\n",
       "      <td>0.007097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hypothesis  feature         x       p_value  p_value_holm  p_value_bh\n",
       "9907   non-null     9907  5.322609  5.114466e-08      0.000511    0.000511\n",
       "9942   non-null     9942  5.022174  2.554492e-07      0.002554    0.001277\n",
       "9941   non-null     9941  4.801528  7.872958e-07      0.007871    0.001968\n",
       "9943   non-null     9943  4.831177  6.786409e-07      0.006785    0.001968\n",
       "9976   non-null     9976  4.674271  1.475000e-06      0.014744    0.002950\n",
       "9964   non-null     9964  4.589147  2.225301e-06      0.022242    0.003709\n",
       "9974   non-null     9974  4.515318  3.161090e-06      0.031592    0.004516\n",
       "9990   non-null     9990  4.433625  4.633087e-06      0.046298    0.005637\n",
       "9909   non-null     9909  4.414029  5.073215e-06      0.050692    0.005637\n",
       "9995   non-null     9995  4.301102  8.497538e-06      0.084890    0.007097"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['p_value_bh'] = adjust_pvalues(df['p_value'], 'fdr_bh')\n",
    "df[['hypothesis', 'feature', 'x', 'p_value', 'p_value_holm', 'p_value_bh']] \\\n",
    "    .sort_values('p_value_bh') \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e557db8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hypothesis  is_p_value_holm_significant\n",
       "non-null    False                            92\n",
       "            True                              8\n",
       "null        False                          9900\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_p_value_holm_significant'] = df['p_value_holm'] <= 0.05\n",
    "df.groupby(['hypothesis', 'is_p_value_holm_significant']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3019c5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hypothesis  is_p_value_bh_significant\n",
       "non-null    False                          67\n",
       "            True                           33\n",
       "null        False                        9898\n",
       "            True                            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_p_value_bh_significant'] = df['p_value_bh'] <= 0.05\n",
    "df.groupby(['hypothesis', 'is_p_value_bh_significant']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1142a5",
   "metadata": {},
   "source": [
    "BH procedure now correctly flagged 33 out of 100 non-null features as significant - an improvement from the 8 with the Holm's correction. However, it also flagged 2 null features as significant. So, out of the 35 features flagged as significant, the fraction of incorrect features is: 2 / 33 = 0.06 so 6%.\n",
    "\n",
    "Note that in this case we have 6% FDR rate, even though we aimed to controll it at 5%. FDR will be controlled at a 5% rate *on average*: sometimes it may be lower and sometimes it may be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc8b17",
   "metadata": {},
   "source": [
    "## Benjamini-Yekutieli procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647aed1c",
   "metadata": {},
   "source": [
    "Benjamini-Yekutieli (BY) procedure controls FDR regardless of whether tests are independent or not. Again, it is worth noting that all of these procedures try to establish *upper bounds* on FDR (or FWER), so they may be less or more conservative. Let's compare the BY procedure with a BH and Holm procedures above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1674c7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>feature</th>\n",
       "      <th>x</th>\n",
       "      <th>p_value</th>\n",
       "      <th>p_value_holm</th>\n",
       "      <th>p_value_bh</th>\n",
       "      <th>p_value_by</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9907</td>\n",
       "      <td>5.322609</td>\n",
       "      <td>5.114466e-08</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.005006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9942</td>\n",
       "      <td>5.022174</td>\n",
       "      <td>2.554492e-07</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.012501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9943</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9943</td>\n",
       "      <td>4.831177</td>\n",
       "      <td>6.786409e-07</td>\n",
       "      <td>0.006785</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.019264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9941</td>\n",
       "      <td>4.801528</td>\n",
       "      <td>7.872958e-07</td>\n",
       "      <td>0.007871</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.019264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9976</td>\n",
       "      <td>4.674271</td>\n",
       "      <td>1.475000e-06</td>\n",
       "      <td>0.014744</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.028873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9964</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9964</td>\n",
       "      <td>4.589147</td>\n",
       "      <td>2.225301e-06</td>\n",
       "      <td>0.022242</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>0.036301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9974</td>\n",
       "      <td>4.515318</td>\n",
       "      <td>3.161090e-06</td>\n",
       "      <td>0.031592</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>0.044199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9990</td>\n",
       "      <td>4.433625</td>\n",
       "      <td>4.633087e-06</td>\n",
       "      <td>0.046298</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.055172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9909</td>\n",
       "      <td>4.414029</td>\n",
       "      <td>5.073215e-06</td>\n",
       "      <td>0.050692</td>\n",
       "      <td>0.005637</td>\n",
       "      <td>0.055172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>non-null</td>\n",
       "      <td>9916</td>\n",
       "      <td>4.316007</td>\n",
       "      <td>7.943832e-06</td>\n",
       "      <td>0.079367</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>0.069458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     hypothesis  feature         x       p_value  p_value_holm  p_value_bh   \n",
       "9907   non-null     9907  5.322609  5.114466e-08      0.000511    0.000511  \\\n",
       "9942   non-null     9942  5.022174  2.554492e-07      0.002554    0.001277   \n",
       "9943   non-null     9943  4.831177  6.786409e-07      0.006785    0.001968   \n",
       "9941   non-null     9941  4.801528  7.872958e-07      0.007871    0.001968   \n",
       "9976   non-null     9976  4.674271  1.475000e-06      0.014744    0.002950   \n",
       "9964   non-null     9964  4.589147  2.225301e-06      0.022242    0.003709   \n",
       "9974   non-null     9974  4.515318  3.161090e-06      0.031592    0.004516   \n",
       "9990   non-null     9990  4.433625  4.633087e-06      0.046298    0.005637   \n",
       "9909   non-null     9909  4.414029  5.073215e-06      0.050692    0.005637   \n",
       "9916   non-null     9916  4.316007  7.943832e-06      0.079367    0.007097   \n",
       "\n",
       "      p_value_by  \n",
       "9907    0.005006  \n",
       "9942    0.012501  \n",
       "9943    0.019264  \n",
       "9941    0.019264  \n",
       "9976    0.028873  \n",
       "9964    0.036301  \n",
       "9974    0.044199  \n",
       "9990    0.055172  \n",
       "9909    0.055172  \n",
       "9916    0.069458  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['p_value_by'] = adjust_pvalues(df['p_value'], 'fdr_by')\n",
    "df[['hypothesis', 'feature', 'x', 'p_value', 'p_value_holm', 'p_value_bh', 'p_value_by']] \\\n",
    "    .sort_values('p_value_by') \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "49442961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hypothesis  is_p_value_by_significant\n",
       "non-null    False                          93\n",
       "            True                            7\n",
       "null        False                        9900\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_p_value_by_significant'] = df['p_value_by'] <= 0.05\n",
    "df.groupby(['hypothesis', 'is_p_value_by_significant']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47ebc1",
   "metadata": {},
   "source": [
    "BY procedure is stricter in controlling FDR; in this case even more so than the Holm's procedure for controlling FWER, by flagging only 7 non-null features as significant. Although the main advantage of using it is when we know the data may contain a high number of correlated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7962c",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130c0ab",
   "metadata": {},
   "source": [
    "At the end, the choice of procedure is left to the user and depends on what the analysis is trying to do. Quoting Benjamini, Hochberg (Royal Stat. Soc. 1995):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd769182",
   "metadata": {},
   "source": [
    "> Often the control of the FWER is not quite needed. The control of the FWER is important when a conclusion from the various individual inferences is likely to be erroneous when at least one ofthem is.\n",
    "> \n",
    "> This may be the case, for example, when several new treatments are competing against a standard, and a single treatment is chosen from the set of treatments which are declared significantly better than the standard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474534bb",
   "metadata": {},
   "source": [
    "In other cases, where we may be OK to have some false positives, FDR methods provide less stringent p-value adjustments and may be preferrable if we primarily want to increase the number of true positives that pass a certain p-value threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
